{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Wav2vec2 Huggingface model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from torchaudio.models.wav2vec2.utils import import_huggingface_model\n",
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "\n",
    "AUDIO_MAXLEN = 160000\n",
    "MODEL_OUTDIR = Path(\"/models/wav2vec2/1/\")\n",
    "MODEL_OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "HF_REPO_NAME = \"kresnik/wav2vec2-large-xlsr-korean\"\n",
    "ROOT_DIR = '/opt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 2.31k/2.31k [00:00<00:00, 10.5MB/s]\n",
      "model.safetensors: 100%|██████████| 1.27G/1.27G [01:50<00:00, 11.5MB/s]\n",
      "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "      )\n",
       "      (1-4): 4 x ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (5-6): 2 x ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (feature_projection): FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (aux): Linear(in_features=1024, out_features=1205, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "original = Wav2Vec2ForCTC.from_pretrained(HF_REPO_NAME)\n",
    "imported = import_huggingface_model(original) \n",
    "imported.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conver to FP32 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, AUDIO_MAXLEN, requires_grad=True)\n",
    "\n",
    "torch.onnx.export(\n",
    "        imported,         # model being run\n",
    "         dummy_input,       # model input (or a tuple for multiple inputs)\n",
    "         f\"{MODEL_OUTDIR}/model.onnx\",       # where to save the model\n",
    "         export_params=True,  # store the trained parameter weights inside the model file\n",
    "         opset_version=14,    # the ONNX version to export the model to\n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "         input_names = ['input'],   # the model's input names\n",
    "         output_names = ['output'], # the model's output names\n",
    "         dynamic_axes={\n",
    "            'input' : {\n",
    "                0: 'batch_size',\n",
    "                1: 'input_sequence'\n",
    "                },    \n",
    "            'output' : {\n",
    "                0: 'batch_size',\n",
    "                1: 'output_sequence'\n",
    "                }\n",
    "            \n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/workspace/vocab.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "  d = eval(f.read())\n",
    "\n",
    "res = dict((v, k) for k, v in d.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100000\n",
    "new_rate = 16000\n",
    "AUDIO_MAXLEN = input_size\n",
    "ort_session = onnxruntime.InferenceSession(f'{MODEL_OUTDIR}/model.onnx') # load onnx model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict((v,k) for k,v in d.items())\n",
    "res[69]=\"[PAD]\"\n",
    "res[68]=\"[UNK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x): #\n",
    "  \"\"\"You must call this before padding.\n",
    "  Code from https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/processor.py#L101\n",
    "  Fork TF to numpy\n",
    "  \"\"\"\n",
    "  # -> (1, seqlen)\n",
    "  mean = np.mean(x, axis=-1, keepdims=True)\n",
    "  var = np.var(x, axis=-1, keepdims=True)\n",
    "  return np.squeeze((x - mean) / np.sqrt(var + 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_adjacent(item): # code from https://stackoverflow.com/a/3460423\n",
    "  nums = list(item)\n",
    "  a = nums[:1]\n",
    "  for item in nums[1:]:\n",
    "    if item != a[-1]:\n",
    "      a.append(item)\n",
    "  return ''.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asr(path):\n",
    "    \"\"\"\n",
    "    Code from https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2_onnx.ipynb\n",
    "    Fork TF to numpy\n",
    "    \"\"\"\n",
    "    sampling_rate, data = wavfile.read(path)\n",
    "    samples = round(len(data) * float(new_rate) / sampling_rate)\n",
    "    new_data = sps.resample(data, samples)\n",
    "    speech = np.array(new_data, dtype=np.float32)\n",
    "    speech = _normalize(speech)[None]\n",
    "    padding = np.zeros((speech.shape[0], AUDIO_MAXLEN - speech.shape[1]))\n",
    "    speech = np.concatenate([speech, padding], axis=-1).astype(np.float32)\n",
    "    ort_inputs = {\"input\": speech}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    prediction = np.argmax(ort_outs, axis=-1)\n",
    "    # Text post processing\n",
    "    _t1 = ''.join([res[i] for i in list(prediction[0][0])])\n",
    "    return normalize(''.join([remove_adjacent(j) for j in _t1.split(\"[PAD]\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"/workspace/sound.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conver to FP16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_OUTDIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m MODEL_FP16_OUTDIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/models/wav2vec2_fp16/1/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m MODEL_FP16_OUTDIR\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m model_fp32 \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload_model(\u001b[43mMODEL_OUTDIR\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m model_fp16 \u001b[38;5;241m=\u001b[39m float16\u001b[38;5;241m.\u001b[39mconvert_float_to_float16(model_fp32)\n\u001b[1;32m     11\u001b[0m onnx\u001b[38;5;241m.\u001b[39msave(model_fp16, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_FP16_OUTDIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_OUTDIR' is not defined"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = \"/models/wav2vec2_fp16/1/model.onnx\"\n",
    "MODEL_FP16_OUTDIR = Path(\"/models/wav2vec2_fp16/1/\")\n",
    "MODEL_FP16_OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_fp32 = onnx.load_model(model_path)\n",
    "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
    "onnx.save(model_fp16, f\"{MODEL_FP16_OUTDIR}/model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
