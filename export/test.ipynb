{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Wav2vec2 model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, Wav2Vec2ForCTC\n",
    "from torchaudio.models.wav2vec2.utils import import_huggingface_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 1.84k/1.84k [00:00<00:00, 9.36MB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "pytorch_model.bin: 100%|██████████| 1.26G/1.26G [01:51<00:00, 11.4MB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of the model checkpoint at airesearch/wav2vec2-large-xlsr-53-th were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at airesearch/wav2vec2-large-xlsr-53-th and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "original = Wav2Vec2ForCTC.from_pretrained(\"airesearch/wav2vec2-large-xlsr-53-th\")\n",
    "imported = import_huggingface_model(original) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "      )\n",
       "      (1-4): 4 x ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (5-6): 2 x ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (feature_projection): FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (aux): Linear(in_features=1024, out_features=70, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "input_size = 160000\n",
    "AUDIO_MAXLEN = input_size\n",
    "MODEL_OUTDIR = Path(\"/models/wav2vec2/1/\")\n",
    "MODEL_OUTDIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, input_size, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "        imported,         # model being run\n",
    "         dummy_input,       # model input (or a tuple for multiple inputs)\n",
    "         f\"{MODEL_OUTDIR}/model.onnx\",       # where to save the model\n",
    "         export_params=True,  # store the trained parameter weights inside the model file\n",
    "         opset_version=14,    # the ONNX version to export the model to\n",
    "         do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "         input_names = ['input'],   # the model's input names\n",
    "         output_names = ['output'], # the model's output names\n",
    "         dynamic_axes={\n",
    "            'input' : {\n",
    "                0: 'batch_size',\n",
    "                1: 'input_sequence'\n",
    "                },    \n",
    "            'output' : {\n",
    "                0: 'batch_size',\n",
    "                1: 'output_sequence'\n",
    "                }\n",
    "            \n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-23 15:44:41--  https://www.dropbox.com/s/9kpeh8eodshcqhj/common_voice_th_23646850.wav?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.84.18, 2620:100:6034:18::a27d:5412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.84.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/dl/9kpeh8eodshcqhj/common_voice_th_23646850.wav [following]\n",
      "--2024-01-23 15:44:45--  https://www.dropbox.com/s/dl/9kpeh8eodshcqhj/common_voice_th_23646850.wav\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc90027e405434cb4b55460bc627.dl.dropboxusercontent.com/cd/0/get/CL5n94jmUsw5bzMB2rmocpC-yk5jqRElBegT7gg_BLU_wQbVYwaeQ0TBXlfCHbV3SyQ9x7ZFOtBD165dDUNo165BrtJx3N19pJi9a2aeuT_TCt1LpBwyrnidXjpiFGbyMQwoGNgW6PuA8v8cAKv0m5O2/file?dl=1# [following]\n",
      "--2024-01-23 15:44:46--  https://uc90027e405434cb4b55460bc627.dl.dropboxusercontent.com/cd/0/get/CL5n94jmUsw5bzMB2rmocpC-yk5jqRElBegT7gg_BLU_wQbVYwaeQ0TBXlfCHbV3SyQ9x7ZFOtBD165dDUNo165BrtJx3N19pJi9a2aeuT_TCt1LpBwyrnidXjpiFGbyMQwoGNgW6PuA8v8cAKv0m5O2/file?dl=1\n",
      "Resolving uc90027e405434cb4b55460bc627.dl.dropboxusercontent.com (uc90027e405434cb4b55460bc627.dl.dropboxusercontent.com)... 162.125.84.15, 2620:100:6034:15::a27d:540f\n",
      "Connecting to uc90027e405434cb4b55460bc627.dl.dropboxusercontent.com (uc90027e405434cb4b55460bc627.dl.dropboxusercontent.com)|162.125.84.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 425326 (415K) [application/binary]\n",
      "Saving to: ‘common_voice_th_23646850.wav?dl=1’\n",
      "\n",
      "common_voice_th_236 100%[===================>] 415.36K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-01-23 15:44:47 (11.2 MB/s) - ‘common_voice_th_23646850.wav?dl=1’ saved [425326/425326]\n",
      "\n",
      "--2024-01-23 15:44:47--  https://huggingface.co/airesearch/wav2vec2-large-xlsr-53-th/raw/main/vocab.json\n",
      "Resolving huggingface.co (huggingface.co)... 13.225.131.35, 13.225.131.6, 13.225.131.94, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.225.131.35|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 762 [text/plain]\n",
      "Saving to: ‘vocab.json’\n",
      "\n",
      "vocab.json          100%[===================>]     762  --.-KB/s    in 0s      \n",
      "\n",
      "2024-01-23 15:44:48 (220 MB/s) - ‘vocab.json’ saved [762/762]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/9kpeh8eodshcqhj/common_voice_th_23646850.wav?dl=1\n",
    "!mv common_voice_th_23646850.wav?dl=1 ${ROOT}/sound.wav\n",
    "!wget https://huggingface.co/airesearch/wav2vec2-large-xlsr-53-th/raw/main/vocab.json \n",
    "!mv vocab.json ${ROOT}/vocab.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/workspace/vocab.json\",\"r\",encoding=\"utf-8-sig\") as f:\n",
    "  d = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ฑ': 0, 'ๅ': 1, 'ก': 2, 'ง': 3, 'ฒ': 4, 'ะ': 5, '๊': 6, '้': 7, 'ฌ': 8, 'ซ': 9, 'ด': 10, 'ฯ': 11, 'ใ': 12, 'ึ': 13, 'ญ': 14, '่': 15, 'า': 16, 'ฤ': 17, '๋': 18, 'อ': 19, 'ฬ': 20, 'ท': 21, 'โ': 22, 'ภ': 23, 'ย': 24, '็': 25, 'ล': 26, 'ุ': 27, 'เ': 28, 'ฮ': 29, 'ฝ': 30, 'ป': 31, 'ี': 32, 'บ': 33, 'ฐ': 34, 'ต': 35, 'ถ': 36, 'ศ': 37, 'ฟ': 38, 'ณ': 39, 'ห': 40, 'ร': 41, 'พ': 43, 'ฆ': 44, 'ั': 45, 'ค': 46, 'ว': 47, 'ฏ': 48, 'จ': 49, 'แ': 50, 'ม': 51, 'ฎ': 52, 'ฉ': 53, '์': 54, 'ษ': 55, 'ำ': 56, 'ผ': 57, 'ข': 58, 'ไ': 59, 'ู': 60, 'ื': 61, 'น': 62, 'ช': 63, 'ิ': 64, 'ธ': 65, 'ฃ': 66, 'ส': 67, '|': 42, '[UNK]': 68, '[PAD]': 69}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as sps\n",
    "import os\n",
    "from pythainlp.util import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 100000\n",
    "new_rate = 16000\n",
    "AUDIO_MAXLEN = input_size\n",
    "ort_session = onnxruntime.InferenceSession(f'{MODEL_OUTDIR}/model.onnx') # load onnx model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dict((v,k) for k,v in d.items())\n",
    "res[69]=\"[PAD]\"\n",
    "res[68]=\"[UNK]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x): #\n",
    "  \"\"\"You must call this before padding.\n",
    "  Code from https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/processor.py#L101\n",
    "  Fork TF to numpy\n",
    "  \"\"\"\n",
    "  # -> (1, seqlen)\n",
    "  mean = np.mean(x, axis=-1, keepdims=True)\n",
    "  var = np.var(x, axis=-1, keepdims=True)\n",
    "  return np.squeeze((x - mean) / np.sqrt(var + 1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_adjacent(item): # code from https://stackoverflow.com/a/3460423\n",
    "  nums = list(item)\n",
    "  a = nums[:1]\n",
    "  for item in nums[1:]:\n",
    "    if item != a[-1]:\n",
    "      a.append(item)\n",
    "  return ''.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asr(path):\n",
    "    \"\"\"\n",
    "    Code from https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/notebooks/wav2vec2_onnx.ipynb\n",
    "    Fork TF to numpy\n",
    "    \"\"\"\n",
    "    sampling_rate, data = wavfile.read(path)\n",
    "    samples = round(len(data) * float(new_rate) / sampling_rate)\n",
    "    new_data = sps.resample(data, samples)\n",
    "    speech = np.array(new_data, dtype=np.float32)\n",
    "    speech = _normalize(speech)[None]\n",
    "    padding = np.zeros((speech.shape[0], AUDIO_MAXLEN - speech.shape[1]))\n",
    "    speech = np.concatenate([speech, padding], axis=-1).astype(np.float32)\n",
    "    ort_inputs = {\"input\": speech}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    prediction = np.argmax(ort_outs, axis=-1)\n",
    "    # Text post processing\n",
    "    _t1 = ''.join([res[i] for i in list(prediction[0][0])])\n",
    "    return normalize(''.join([remove_adjacent(j) for j in _t1.split(\"[PAD]\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"/workspace/sound.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3164/2770446858.py:6: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = wavfile.read(path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'บริษัท|เรา|จะ|ต้อง|ปรับตัว|เพื่อ|ใช้งาน|เทคโนโลยี|เหล่านี้'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
